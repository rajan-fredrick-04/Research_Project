{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import spacy \n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"Course-co.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Course Outcomes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CO1</td>\n",
       "      <td>to be able to understand the concept of the ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CO2</td>\n",
       "      <td>to evaluate condition probabilities and condit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CO3</td>\n",
       "      <td>gain the knowledge of applications of discrete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CO4</td>\n",
       "      <td>identify the applications of continuous distri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     No                                    Course Outcomes\n",
       "0  CO1   to be able to understand the concept of the ra...\n",
       "1  CO2   to evaluate condition probabilities and condit...\n",
       "2  CO3   gain the knowledge of applications of discrete...\n",
       "3  CO4   identify the applications of continuous distri..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_outcomes=[]\n",
    "for i in range(len(df)):\n",
    "    verbs=df[' Course Outcomes'].iloc[i]\n",
    "    course_outcomes.append(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to be able to understand the concept of the random variable and expectation for discrete and continuous data',\n",
       " 'to evaluate condition probabilities and conditional expectations',\n",
       " 'gain the knowledge of applications of discrete distributions in Data Science',\n",
       " 'identify the applications of continuous distributions in Data Science']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 'TO'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('concept', 'NN'), ('of', 'IN'), ('the', 'DT'), ('random', 'NN'), ('variable', 'JJ'), ('and', 'CC'), ('expectation', 'NN'), ('for', 'IN'), ('discrete', 'JJ'), ('and', 'CC'), ('continuous', 'JJ'), ('data', 'NNS')]\n",
      "[('to', 'TO'), ('evaluate', 'VB'), ('condition', 'NN'), ('probabilities', 'NNS'), ('and', 'CC'), ('conditional', 'JJ'), ('expectations', 'NNS')]\n",
      "[('gain', 'VB'), ('the', 'DT'), ('knowledge', 'NN'), ('of', 'IN'), ('applications', 'NNS'), ('of', 'IN'), ('discrete', 'JJ'), ('distributions', 'NNS'), ('in', 'IN'), ('Data', 'NNP'), ('Science', 'NN')]\n",
      "[('identify', 'VB'), ('the', 'DT'), ('applications', 'NNS'), ('of', 'IN'), ('continuous', 'JJ'), ('distributions', 'NNS'), ('in', 'IN'), ('Data', 'NNP'), ('Science', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "course_verbs=[]\n",
    "verb=['VB','VBP','VBD','VBG','VBN']\n",
    "for i in range(len(course_outcomes)):\n",
    "    review=course_outcomes[i]\n",
    "    review=review.split()\n",
    "    review= nltk.pos_tag(review)\n",
    "    print(review)\n",
    "    review=[word for word,tag in review if tag in verb]\n",
    "    course_verbs.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['be', 'understand'], ['evaluate'], ['gain'], ['identify']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blooms=pd.read_excel(\"Blooms_index.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blooms['Verb '] = df_blooms['Verb '].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level:</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Examples of Appropriate Assessments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remembering: can the student recall or remembe...</td>\n",
       "      <td>[Recall, Recognize, Identify]</td>\n",
       "      <td>Objective test items such as fill-in-the-blank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Understanding: can the student explain ideas o...</td>\n",
       "      <td>[Interpret, Exemplify, Classify, Summarize, In...</td>\n",
       "      <td>Activities such as papers, exams, problem sets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Applying: can the student use the information ...</td>\n",
       "      <td>[Apply, Execute, Implement]</td>\n",
       "      <td>Activities such as problem sets, performances,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analyzing: can the student distinguish between...</td>\n",
       "      <td>[Analyze, Differentiate, Organize, Attribute]</td>\n",
       "      <td>Activities such as case studies, critiques, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Evaluating: can the student justify a stand or...</td>\n",
       "      <td>[Evaluate, Check, Critique, Assess]</td>\n",
       "      <td>Activities such as journals, diaries, critique...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Creating: can the student create new product o...</td>\n",
       "      <td>[Create, Generate, Plan, Produce, Design]</td>\n",
       "      <td>Activities such as research projects, musical ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Level:   \\\n",
       "0  Remembering: can the student recall or remembe...   \n",
       "1  Understanding: can the student explain ideas o...   \n",
       "2  Applying: can the student use the information ...   \n",
       "3  Analyzing: can the student distinguish between...   \n",
       "4  Evaluating: can the student justify a stand or...   \n",
       "5  Creating: can the student create new product o...   \n",
       "\n",
       "                                               Verb   \\\n",
       "0                      [Recall, Recognize, Identify]   \n",
       "1  [Interpret, Exemplify, Classify, Summarize, In...   \n",
       "2                        [Apply, Execute, Implement]   \n",
       "3      [Analyze, Differentiate, Organize, Attribute]   \n",
       "4                [Evaluate, Check, Critique, Assess]   \n",
       "5          [Create, Generate, Plan, Produce, Design]   \n",
       "\n",
       "                 Examples of Appropriate Assessments  \n",
       "0  Objective test items such as fill-in-the-blank...  \n",
       "1  Activities such as papers, exams, problem sets...  \n",
       "2  Activities such as problem sets, performances,...  \n",
       "3  Activities such as case studies, critiques, la...  \n",
       "4  Activities such as journals, diaries, critique...  \n",
       "5  Activities such as research projects, musical ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_course_verbs = [verb[0] for verb in course_verbs if verb]\n",
    "\n",
    "all_verbs = df_blooms['Verb '].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Recall', 'Recognize', 'Identify'],\n",
       " ['Interpret',\n",
       "  'Exemplify',\n",
       "  'Classify',\n",
       "  'Summarize',\n",
       "  'Infer',\n",
       "  'Compare',\n",
       "  'Explain'],\n",
       " ['Apply', 'Execute', 'Implement'],\n",
       " ['Analyze', 'Differentiate', 'Organize', 'Attribute'],\n",
       " ['Evaluate', 'Check', 'Critique', 'Assess'],\n",
       " ['Create', 'Generate', 'Plan', 'Produce', 'Design']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Objective test items such as fill-in-the-blank...\n",
       "1    Activities such as papers, exams, problem sets...\n",
       "2    Activities such as problem sets, performances,...\n",
       "3    Activities such as case studies, critiques, la...\n",
       "4    Activities such as journals, diaries, critique...\n",
       "5    Activities such as research projects, musical ...\n",
       "Name: Examples of Appropriate Assessments, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blooms['Examples of Appropriate Assessments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessments for course verb 'be': []\n",
      "Assessments for course verb 'evaluate': []\n",
      "Assessments for course verb 'gain': []\n",
      "Assessments for course verb 'identify': ['Objective test items such as fill-in-the-blank, matching, labeling, or multiple-choice questions that require students to: recall or recognize terms, facts, and concepts •']\n"
     ]
    }
   ],
   "source": [
    "flattened_course_verbs = [verb[0] for verb in course_verbs if verb]\n",
    "\n",
    "# Step 2: Flatten the verbs from the DataFrame\n",
    "flattened_df_verbs = [' '.join(verbs) for verbs in df_blooms['Verb ']]\n",
    "\n",
    "# Step 3: Vectorize both course_verbs and df_verbs separately using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorize the flattened course verbs\n",
    "course_verb_tfidf = vectorizer.fit_transform(flattened_course_verbs)\n",
    "\n",
    "# Vectorize the flattened DataFrame verbs\n",
    "df_verb_tfidf = vectorizer.transform(flattened_df_verbs)\n",
    "\n",
    "# Step 4: Calculate cosine similarity between the course verbs and DataFrame verbs\n",
    "cosine_sim = cosine_similarity(course_verb_tfidf, df_verb_tfidf)\n",
    "\n",
    "# Step 5: Retrieve assessments for closely related verbs\n",
    "matching_assessments = {}\n",
    "threshold = 0.3  # Set a similarity threshold\n",
    "\n",
    "for i, course_verb in enumerate(flattened_course_verbs):\n",
    "    matching_assessments[course_verb] = []\n",
    "    \n",
    "    # Compare with the DataFrame verbs\n",
    "    for j in range(len(df)):\n",
    "        sim_score = cosine_sim[i, j]  # Get similarity score with the DataFrame verb\n",
    "        if sim_score > threshold:\n",
    "            matching_assessments[course_verb].append(df_blooms.iloc[j]['Examples of Appropriate Assessments'])\n",
    "\n",
    "# Print results\n",
    "for verb, assessments in matching_assessments.items():\n",
    "    print(f\"Assessments for course verb '{verb}': {assessments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessments for course verb 'be': ['Objective test items such as fill-in-the-blank, matching, labeling, or multiple-choice questions that require students to: recall or recognize terms, facts, and concepts •']\n",
      "Assessments for course verb 'evaluate': ['Objective test items such as fill-in-the-blank, matching, labeling, or multiple-choice questions that require students to: recall or recognize terms, facts, and concepts •', 'Activities such as case studies, critiques, labs, papers, projects, debates, or concept maps that require students to: discriminate or select relevant and irrelevant parts • determine how elements function together • determine bias, values, or underlying intent in presented material •', 'Activities such as journals, diaries, critiques, problem sets, product reviews, or studies that require students to: test, monitor, judge, or critique readings, performances, or products against established • criteria or standards']\n",
      "Assessments for course verb 'gain': ['Objective test items such as fill-in-the-blank, matching, labeling, or multiple-choice questions that require students to: recall or recognize terms, facts, and concepts •']\n",
      "Assessments for course verb 'identify': ['Objective test items such as fill-in-the-blank, matching, labeling, or multiple-choice questions that require students to: recall or recognize terms, facts, and concepts •', 'Activities such as case studies, critiques, labs, papers, projects, debates, or concept maps that require students to: discriminate or select relevant and irrelevant parts • determine how elements function together • determine bias, values, or underlying intent in presented material •', 'Activities such as journals, diaries, critiques, problem sets, product reviews, or studies that require students to: test, monitor, judge, or critique readings, performances, or products against established • criteria or standards']\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model and tokenizer\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "flattened_course_verbs2 = [verb[0] for verb in course_verbs if verb]\n",
    "flattened_df_verbs2 = [' '.join(verbs) for verbs in df_blooms['Verb ']]\n",
    "\n",
    "# Function to get BERT embedding for a word or phrase\n",
    "def get_bert_embedding(text):\n",
    "    try:\n",
    "        # Tokenize and obtain input IDs\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=10  # Adjust max_length based on expected input length\n",
    "        )\n",
    "        \n",
    "        # Pass input_ids and attention_mask to the model\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        \n",
    "        # Return the mean of the embeddings across tokens\n",
    "        return outputs.last_hidden_state.mean(dim=1).detach().numpy().flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text '{text}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Get embeddings for course verbs and DataFrame verbs using BERT\n",
    "course_verb_vectors2 = np.array([vec for verb in flattened_course_verbs2 \n",
    "                                 if (vec := get_bert_embedding(verb)) is not None])\n",
    "df_verb_vectors2 = np.array([vec for verb in flattened_df_verbs2 \n",
    "                             if (vec := get_bert_embedding(verb)) is not None])\n",
    "\n",
    "# Check for empty embedding arrays\n",
    "if course_verb_vectors2.size == 0 or df_verb_vectors2.size == 0:\n",
    "    print(\"Error: One or both of the embedding arrays are empty.\")\n",
    "else:\n",
    "    # Calculate cosine similarity between course verb vectors and DataFrame verb vectors\n",
    "    cosine_sim2 = cosine_similarity(course_verb_vectors2, df_verb_vectors2)\n",
    "\n",
    "# Retrieve assessments for closely related verbs\n",
    "matching_assessments2 = {}\n",
    "threshold2 = 0.7  # Set a similarity threshold\n",
    "\n",
    "for i, course_verb in enumerate(flattened_course_verbs2):\n",
    "    matching_assessments2[course_verb] = []\n",
    "    if i < len(course_verb_vectors2):  # Check if index is valid\n",
    "        for j in range(len(df_verb_vectors2)):\n",
    "            sim_score = cosine_sim2[i, j]\n",
    "            if sim_score > threshold2:\n",
    "                matching_assessments2[course_verb].append(df_blooms.iloc[j]['Examples of Appropriate Assessments'])\n",
    "\n",
    "# Print results\n",
    "for verb, assessments in matching_assessments2.items():\n",
    "    print(f\"Assessments for course verb '{verb}': {assessments}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Recall Recognize Identify',\n",
       " 'Interpret Exemplify Classify Summarize Infer Compare Explain',\n",
       " 'Apply Execute Implement',\n",
       " 'Analyze Differentiate Organize Attribute',\n",
       " 'Evaluate Check Critique Assess',\n",
       " 'Create Generate Plan Produce Design']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load SpaCy's pre-trained medium-sized model with word vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Step 1: Flatten the course verbs\n",
    "flattened_course_verbs3 = [verb[0] for verb in course_verbs if verb]\n",
    "\n",
    "# Step 2: Flatten the verbs from the DataFrame\n",
    "flattened_df_verbs3 = [' '.join(verbs) for verbs in df_blooms['Verb ']]\n",
    "flattened_df_verbs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to get the SpaCy vector for a word\n",
    "def get_word_vector(word):\n",
    "    doc = nlp(word)\n",
    "    if doc.has_vector:  # Check if the word has a vector\n",
    "        return doc.vector\n",
    "    else:\n",
    "        return np.zeros(nlp.vocab.vectors_length)  # Return a zero vector if word not in vocabulary\n",
    "\n",
    "# Step 3: Convert course verbs and DataFrame verbs to vectors using SpaCy embeddings\n",
    "course_verb_vectors3 = np.array([get_word_vector(verb) for verb in flattened_course_verbs3])\n",
    "df_verb_vectors3 = np.array([get_word_vector(verb) for verb in flattened_df_verbs3])\n",
    "\n",
    "# Step 4: Calculate cosine similarity between the course verb vectors and DataFrame verb vectors\n",
    "cosine_sim3 = cosine_similarity(course_verb_vectors3, df_verb_vectors3)\n",
    "\n",
    "# Step 5: Retrieve assessments for closely related verbs\n",
    "matching_assessments3 = {}\n",
    "threshold3 = 0.3  # Set a similarity threshold\n",
    "\n",
    "for i, course_verb in enumerate(flattened_course_verbs3):\n",
    "    matching_assessments3[course_verb] = []\n",
    "    \n",
    "    # Compare with the DataFrame verbs\n",
    "    for j in range(len(df_verb_vectors3)):\n",
    "        sim_score = cosine_sim3[i, j]  # Get similarity score with the DataFrame verb\n",
    "        if sim_score > threshold3:\n",
    "            matching_assessments3[course_verb].append(df_blooms.iloc[j]['Examples of Appropriate Assessments'])\n",
    "\n",
    "# Print results\n",
    "for verb, assessments in matching_assessments3.items():\n",
    "    print(f\"Verbs:{verb}\")\n",
    "    print(f\"Assessments for course verb '{verb}': {assessments}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
